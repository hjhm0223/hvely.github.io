{"componentChunkName":"component---src-templates-single-post-js","path":"/posts/apache-spark-1-개요/","result":{"data":{"post":{"frontmatter":{"meta":{"title":"test meta title","description":"test meta description","noindex":null,"canonicalLink":null},"title":"Apache Spark (1) - 개요","template":"SinglePost","subtitle":null,"date":"April 1st, 2020","categories":[{"category":"Study"}]},"html":"<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Apache_Spark_logo.svg/1200px-Apache_Spark_logo.svg.png\" alt=\"spark\"></p>\n<h2>개요</h2>\n<p><code>Apache Spark</code>는 범용적이면서도 빠른 속도로 작업을 수행할 수 있도록 설계한 <code>클러스터용</code> 연산 플랫폼이자 스트림 처리를 효과적으로 수행하는 인-메모리 방식의 분산 처리 시스템이다.</p>\n<ul>\n<li><code>인메모리 컴퓨팅</code>: 기존의 디스크 기반 컴퓨팅과는 달리 데이터를 하드디스크에 저장하고 관리하는 것이 아니라 전체 데이터를 메모리에 적재하여 사용하는 것을 의미한다. 조금 더 기술적인 정의는 컴퓨팅 시스템의 공식 기록 시스템으로 하드디스크를 사용하는 것이 아니라 메모리를 사용하는 것이라 할 수 있다. 기존 디스크 기반 컴퓨팅에서는 데이터를 하드디스크에 보관/처리하고, 빈번하게 사용되는 일부 데이터를 메모리에 캐싱하는 구조라고 하면, 인메모리 컴퓨팅은 데이터를 메모리에 보관/처리하고 하드디스크를 데이터의 안전한 보관하기 위해 사용하는 구조다.</li>\n<li><code>Spark Dataframe</code>: 관계형 데이터베이스의 테이블에서 칼럼 이름으로 구성된 변경 불가능한 분산 데이터 컬렉션이다.</li>\n<li>Zeppelin: <a href=\"https://zeppelin.apache.org/\">https://zeppelin.apache.org/</a></li>\n</ul>\n<h2>Apache Spark란?</h2>\n<p><img src=\"https://blog.kakaocdn.net/dn/VFS3T/btq27Yf4c6U/SZazHv7jY7IDtZamav6VV0/img.png\" alt=\"image\"></p>\n<ul>\n<li><code>인메모리</code> 기반의 대용량 데이터 고속 처리 엔진이다.</li>\n<li>오픈 소스 범용 분산 클러스터 컴퓨팅 프레임워크이다.</li>\n<li>최초 데이터 로드와 최종 결과 저장시에만 디스크를 사용한다.</li>\n<li>메모리에 분산 저장하고, 병렬 처리 구조이다.</li>\n</ul>\n<h3>Spark 소개 및 특징</h3>\n<ul>\n<li>Hadoop vs Spark</li>\n<li>디스크 입출력 방식을 인-메모리 데이터 처리 방식으로 전환</li>\n<li>기존 맵리듀스 디스크 입출력 방식보다 평균 10~100배 정도의 <code>속도 향상</code></li>\n<li>기존 디스크 입출력에 대한 지연 시간 개선</li>\n<li>메모리를 사용하여 반복적인 작업이나 스트리밍 데이터를 효율적으로 처리</li>\n<li>빅데이터 애플리케이션에 필요한 대부분의 기능을 지원</li>\n<li>맵리듀스와 유사한 일괄 처리 기능</li>\n<li>실시간 데이터 처리 기능 (Spark Streaming)</li>\n<li>SQL과 유사한 정형 데이터 처리 기능 (Spark SQL)</li>\n<li>그래프 알고리즘 (Spark GraphX)</li>\n<li>머신러닝 알고리즘 (Spark MLlib)</li>\n<li>아키텍처 구조</li>\n</ul>\n<p><img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&#x26;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FlLXNZ%2FbtqVcrWGj6d%2FeMIYlXgLtLCuOlNFjLFEm0%2Fimg.png\" alt=\"image\"></p>\n<h4>Spark 특징</h4>\n<ul>\n<li>인메모리 기반의 빠른 처리</li>\n<li>다양한 언어(Java, Scala, Python, R, SQL)을 통한 사용의 편의성</li>\n<li>SQL, Streaming, 머신러닝, 그래프 연산 등 다양한 컴포넌트 제공</li>\n<li>YARN, Mesos, Kubernetes 등 다양한 클러스터에서 동작 가능</li>\n<li>HDFS(하둡 분산파일 시스템), Cassandra, HBase 등 다양한 파일 포맷 지원</li>\n</ul>\n<h4>Spark 지원언어</h4>\n<ul>\n<li>Spark 프로그래밍</li>\n<li>Spark Application은 Scala, Java, Python, R로 구현</li>\n<li>각 언어로 <code>스파크 SQL</code>을 실행</li>\n<li>스칼라</li>\n<li><code>객체 지향 언어</code>의 특징과 <code>함수형 언어</code>의 특징을 함께 가지는 다중 패러다임 프로그래밍 언어</li>\n<li>스칼라는 자바 가상머신(JVM)에서 동작하는 JVML언어</li>\n<li>JVML(Java Virtual Machine Language) 언어는 자바 가상머신 위에서 동작하는 언어들로 Scala, Kotlin, Groovy 등</li>\n<li>파이썬, Java, R</li>\n<li>스파크 쉘: 데이터를 대화식으로 분석하는 강력한 도구인 대화형 쉘을 제공</li>\n<li>PySpark</li>\n</ul>\n<h4>Spark 유사 플랫폼 비교</h4>\n<ul>\n<li>\n<p>스파크와 하둡과의 비교</p>\n<ul>\n<li>스파크의 <code>메모리 내</code> 데이터 고속 처리 엔진은 특정 상황, 스테이지 간 다중 스테이지 작업과 비교 시 맵리듀스에 비해 최대 100배 더 빠르다.</li>\n<li>Apache Spark API는 맵리듀스와 다른 아파치 하둡 구성 요소에 비해 <code>개발자 친화적 API</code>를 제공하여 분산 처리 엔진의 복잡성 대부분을 간단한 메서드로 처리하도록 한다.</li>\n<li>아파치 스파크의 중심은 <code>컴퓨팅 클러스터</code>로 분할 가능한 <code>불변적 객체 컬렉션</code>을 나타내는 프로그래밍 추상화, 즉 <code>탄력적 분산 데이터 집합(Resilient Distributed Dataset, RDD) 개념</code>이다.</li>\n<li>스파크 SQL은 구조적 데이터 처리에 초점을 두며, R과 Python에서 차용한 <code>데이터프레임</code>을 사용한다.</li>\n<li>스파크 SQL은 표준 SQL 지원 외에 기본적으로 지원되는 JSON, HDFS, 아파치 하이브, JDBC, 아파치 ORC, <code>아파치 파케이(Parquet)</code>을 포함한 다른 데이터 저장소에서의 읽기와 쓰기를 위한 표준 인터페이스도 제공한다.</li>\n</ul>\n</li>\n</ul>\n<h3>Apache Spark 구조</h3>\n<ul>\n<li>지원하는 Open Source Ecosystem</li>\n</ul>\n<p><img src=\"https://databricks.com/wp-content/uploads/2019/02/largest-open-source-apache-spark.png\" alt=\"image\"></p>\n<h4>Spark Components</h4>\n<p><img src=\"https://blog.kakaocdn.net/dn/p5OdB/btqAZNZtkHQ/WL2wlmkWmJ67XJVOJXxkw1/img.gif\" alt=\"image\"></p>\n<ul>\n<li>\n<p><code>Spark Core</code></p>\n<ul>\n<li>메모리 기반의 분산 <code>클러스터</code> 컴퓨팅 환경</li>\n<li>스파크 전체의 기초가 되는 <code>기초 분산 작업 처리</code></li>\n<li><code>장애복구</code>, 네트워킹, 보안, 스케줄링 및 데이터 셔플링 등 기본 기능을 제공</li>\n<li>분산 데이터 컬렉션(dataset)을 추상화한 객체인 RDD(Resilient Distributed Dataset)로 다양한 연산 및 변환 메소드를 제공</li>\n<li>스파크 코어는 HDFS, GlusterFS, 아마존 S3 등 다양한 파일 시스템에 접근 가능</li>\n<li>스파크 잡과 다른 스파크 컴포넌트에 필요한 기본 기능을 제공</li>\n</ul>\n</li>\n<li>\n<p><code>Spark SQL</code></p>\n<ul>\n<li>스파크와 하이브 SQL이 지원하는 SQL을 사용해 <code>대규모 분산 정형 데이터 처리</code></li>\n<li>JSON, Parquet 파일, RDB 테이블, 하이블 테이블 등 다양한 정형 데이터 읽기 및 쓰기 가능</li>\n<li>DataFrame과 Dataset에 적용된 연산을 일정 시점에 RDD 연산으로 변환해 일반 스파크 잡으로 실행</li>\n</ul>\n</li>\n<li>\n<p><code>Spark Streaming</code></p>\n<ul>\n<li>실시간 스트리밍 데이터를 처리하는 프레임워크</li>\n<li>HDFS, Apache Kafka, Apache Flume, 트위터, ZeroMQ와 더불어 커스텀 리소스도 사용 가능</li>\n<li>이산 스트림(Discretized Stream, DStream) 방식으로 스트리밍 데이터를 표현하는데, 가장 마지막 타임 윈도 안에 유입된 데이터를 <code>RDD</code>로 구성해 주기적으로 생성</li>\n<li>다른 스파크 컴포넌트와 함께 사용할 수 있어 실시간 데이터 처리를 머신러닝 작업, SQL 작업, 그래프 연산 등을 통합 가능</li>\n</ul>\n</li>\n<li>Ignite: 속도가 더 개선된 데이터 처리 엔진</li>\n<li>\n<p>Spark MLlib</p>\n<ul>\n<li>머신러닝 알고리즘 라이브러리</li>\n<li>RDD 또는 DataFrame의 dataset을 변환하는 머신 러닝 모델을 구현</li>\n</ul>\n</li>\n<li>\n<p>Spark GraphX</p>\n<ul>\n<li>그래프는 정점과 두 정점을 잇는 간선으로 구성된 데이터 구조</li>\n<li>그래프 RDD(EdgeRDD 및 VertexRDD) 형태의 그래프 구조를 만들 수 있는 기능을 제공</li>\n<li>그래프 데이터는 Vertex와 Edge로 구성</li>\n</ul>\n</li>\n</ul>\n<h4>Spark 동작 구조</h4>\n<p><img src=\"uploads/6c57d9145281f7534d8519fbb40fbdfd/image.png\" alt=\"image\"></p>\n<ul>\n<li>\n<p>Cluster Manager</p>\n<ul>\n<li>클러스터에 1개 존재</li>\n<li>여러 대의 서버로 구성된 클러스터 환경에서 다수의 Application이 함께 구동될수 있게 Application 간의 CPU나 메모리, 디스크와 같은 전반적인 <code>컴퓨팅 자원 관리</code></li>\n</ul>\n</li>\n<li><code>스케줄링</code> 담당</li>\n<li>Standalone: 스파크가 독립적으로 작동할 수 있음</li>\n<li>YARN: 하둡의 리소스 매니저(하둡과 스파크는 상호 독립적)</li>\n<li><code>Mesos</code>: 분산 커널 시스템</li>\n<li><code>Kubernetes</code>: Orchestration 도구</li>\n<li>\n<p>Driver</p>\n<ul>\n<li>SparkContext와 RDD 생성</li>\n<li>Transformation과 Action을 생성</li>\n<li>사용자 코드를 실행, main()문과 같은 역할을 하는 프로세스</li>\n</ul>\n</li>\n<li>\n<p>SparkContext</p>\n<ul>\n<li>어떻게 클러스터에 접근할 수 있는지 알려주는 object</li>\n<li>Worker Node 내 Executor간 통신이 발생</li>\n<li>Driver는 Executor에게 Task를 할당</li>\n<li>Executor에서는 Task를 수행하고 종료시 Driver에게 결과 값을 주게 됨</li>\n</ul>\n</li>\n<li>\n<p>Worker Node</p>\n<ul>\n<li>실제 작업을 수행하는 노드</li>\n</ul>\n</li>\n<li>\n<p>Executor</p>\n<ul>\n<li>주어진 스파크 작업의 개별 태스크들을 실행하는 작업실행 프로세스</li>\n<li>애플리케이션을 구성하는 작업을 실행 -> 드라이버에 결과를 전송</li>\n<li>사용자 프로그램에서 캐시하는 RDD를 저장하기 위한 메모리 저장소 제공</li>\n</ul>\n</li>\n<li>Task: executor에 할당되는 작업의 단위</li>\n</ul>\n<h4>Spark 프로그래밍 모델</h4>\n<ul>\n<li>\n<p>Spark의 처리 모델 RDD</p>\n<ul>\n<li>스파크 내부에 존재하는 분산 데이터에 대한 모델, <code>기본 데이터 구조</code></li>\n<li>단순히 값으로 표현되는 데이터만 가리키는 것이 아니라 <code>데이터를 다루는 방법까지 포함</code>하는 일종의 클래스와 같은 개념</li>\n<li>스파크에서 내부적으로 연산하는 데이터들은 모두 RDD 타입으로 처리</li>\n<li>RDD는 대량의 데이터를 요소로 가지는 분산 컬렉션</li>\n<li>RDD는 여러 머신으로 구성된 클러스터 환경에서의 분산처리를 전제로 설계되었으며, 내부는 <code>파티션</code>이라는 단위로 구분</li>\n<li>RDD는 배열/리스트 등의 요소를 저장하며, 내부는 여러 개의 파티션(분산 처리 단위)으로 구분</li>\n<li>사용자는 HDFS 등의 분산 파일시스템에 있는 파일 내용을 RDD로 로드하고, RDD를 가공하는 대량의 데이터를 분산처리</li>\n<li>Immutable(변경이 불가능한 데이터셋): RDD 복구 가능</li>\n</ul>\n</li>\n<li>\n<p>RDD 연산</p>\n<ul>\n<li>Transformation</li>\n<li>주어진 스파크 작업의 개별 태스크들을 실행하는 작업실행 프로세스</li>\n<li>기존의 RDD 내용이 바뀌는 것이 아니라 새로운 RDD가 생성</li>\n<li>즉, 기존의 RDD와 새로운 RDD 2개가 각각 존재</li>\n<li>Action</li>\n<li>RDD의 요소들을 이용해 어떤 결과값을 얻어내는 연산</li>\n<li>return 값이 RDD가 아닌 값(다른 타입의 데이터)</li>\n<li>Partition</li>\n<li>하나의 RDD는 여러 개의 파티션으로 나뉘어짐</li>\n<li>성능에 유효한 영향을 줌</li>\n<li>파티션의 갯수, 파티셔너는 선택 가능</li>\n<li>기본 파티셔너(Hash, range) 외에도 사용자가 정의한 파티셔너를 사용 가능</li>\n</ul>\n</li>\n</ul>\n<h3>Apache Spark 운용</h3>\n<h4>Spark 설치 및 환경설정</h4>\n<ul>\n<li>\n<p>Spark 운용 특징</p>\n<ul>\n<li>대량의 데이터를 고속 병렬분산처리</li>\n<li>스토리지 I/O와 네트워크 I/O를 최소화하도록 처리</li>\n<li>동일한 데이터에 대한 변환처리가 연속으로 이루어지는 경우 머신러닝처럼 결과셋을 여러번 반복하여 처리 고속화</li>\n<li>한 대의 노드로 처리할 수 있는 용량보다 더 큰 데이터셋에 대해 시행착오 가능한 환경 제공</li>\n<li><code>배치처리, 스트림처리, SQL 처리</code>와 같은 서로 다른 형태의 애플리케이션을 하나의 환경에서 통합해 이용할 수 있는 환경</li>\n<li>스파크 코어는 데이터소스로 HDFS뿐만 아니라 Hive, HBase, PostgreSQL, MySQL, CSV 파일 등도 처리</li>\n</ul>\n</li>\n<li>다운로드 사이트: <a href=\"https://spark.apache.org/downloads.html\">https://spark.apache.org/downloads.html</a></li>\n<li>설치</li>\n</ul>\n<pre><code>- wget http://www.apache.org/dyn/closer.lua/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n\n- 압축 풀기\ntar -xvzf spark-2.4.5-bin-hadoop2.7.tgz\n\n- symbolic link\nln --s spark-2.4.5-bin-hadoop2.7 spark\n</code></pre>\n<ul>\n<li>Spark 환경설정</li>\n</ul>\n<pre><code>- 계정설정 ~/.bashrc\nexport SPARK_HOME=/home/계정/spark\nexport PATH=$PATH:$SPARK_HOME/bin\n\n- 설정된 내용 적용\nsource ~/.bashrc\n</code></pre>\n<ul>\n<li>각 언어별 실행 스크립트</li>\n<li>Spark/bin directory에 다음과 같이 각 언어를 실행할 수 있는 스크립트 확인</li>\n</ul>\n<pre><code>- bin 폴더 실행 파일\nrwxr-xr-x 1 root root 3265 Nov 2019 pyspark //파이썬\nrwxr-xr-x 1 root root 1049 Nov 16 2019 sparkR //R\nrwxr-xr-x 1 root root 3026 Nov 16 2019 spark-shell //scala repl\nrwxr-xr-x 1 root root 1075 Nov 16 2019 spark-sql //scala on sql\nrwxr-xr-x 1 root root 1050 Nov 16 2019 spark-submit //scala jar\n</code></pre>\n<ul>\n<li>Spark Application 제출(spark-submit)</li>\n<li>스칼라나 자바로 작성한 스파크 애플리케이션을 jar파일로 만들어서 spark-submit을 이용해 실행</li>\n</ul>\n<pre><code>${SPARK_HOME}/bin/spark-submit \\\n--class &#x3C;main-class> \\\n--master &#x3C;master-url> \\\n--deploy-mode &#x3C;deploy-mode> \\\n--conf &#x3C;key>=&#x3C;value> \\\n...\n</code></pre>\n<ul>\n<li>예시</li>\n</ul>\n<pre><code>{SPARK_HOME}/bin/spark-submit --master spark:/host:7077 --executor-memory 10g myscript.py\n</code></pre>\n<h4>Spark Shell</h4>\n<ul>\n<li>Spark 애플리케이션 REPL 처리</li>\n<li>REPL 환경을 이용한 작업 처리도 지원</li>\n<li>spark-shell, pyspark를 실행하면 REPL 환경에서 interactively 작업 처리</li>\n<li>스칼라를 이용한 처리는 spark-shell 이용</li>\n<li>\n<p>Spark Scala</p>\n<ul>\n<li>객체 지향 언어의 특징과 함수형 언어의 특징을 함께 가지는 다중 패러다임 프로그래밍 언어</li>\n<li>Scala가 성능과 동시처리성 측면에서 유리</li>\n<li>스칼라는 자바 가상머신(JVM)에서 동작하는 JVML언어</li>\n<li>스칼라는 스칼라 컴파일러를 통해 스칼라 코드를 바이트 코드로 변환하고, 바이트 코드는 JVM상에서 자바와 동일하세 실행</li>\n<li>스칼라는 변경 불가능한 Immutable 변수 (RDD의 특징)</li>\n</ul>\n</li>\n</ul>\n<h4>PySpark</h4>\n<ul>\n<li>\n<p>PySpark 개요</p>\n<ul>\n<li>스파크의 메인 추상화는 RDD</li>\n<li>PySpark는 apache spark를 python에서 실행하기 위한 api</li>\n<li>PySpark 스크립트는 대화형 Python shell</li>\n<li>PySpark는 Python의 minor version과 동일한 version을 필요</li>\n<li>기본 파이선 버전을 <code>PATH</code>에 걸어주는 작업이 필요</li>\n<li>특정 버전을 설정하고자 한다면 PYSPARK_PYTHON의 값 설정</li>\n</ul>\n</li>\n</ul>\n<pre><code>$PYSPARK_PYTHON=python3.4 bin/pyspark\n</code></pre>\n<ul>\n<li>PySpark 사용</li>\n<li>스파크 프로그램의 시작은 SparkContext를 생성하는 것으로 시작</li>\n<li>PySpark는 더 많은 일반 spark-submit script를 호출</li>\n<li>PySpark 쉘은 SparkContext가 이미 생성되어 있는 인터프리터</li>\n</ul>\n<pre><code>$./bin/pyspark --master local[4] --py-files code.py\n</code></pre>\n<h3>Reference</h3>\n<p>인메모리 컴퓨팅: <a href=\"https://blog.lgcns.com/176\">https://blog.lgcns.com/176</a></p>","id":"46426481-29ee-5d00-aa6c-cf05d52490f6"},"allPosts":{"edges":[{"node":{"id":"7f695fca-e5ce-5c55-970b-e8855f99b61a"},"next":{"fields":{"slug":"/posts/java-script-1-개요/"},"frontmatter":{"title":"JavaScript (1) - 개요"}},"previous":null},{"node":{"id":"9f46ad6e-455d-53e9-8d4c-4794e5af39dd"},"next":{"fields":{"slug":"/posts/java-script-2-문법과-자료형/"},"frontmatter":{"title":"JavaScript (2) - 문법과 자료형"}},"previous":{"fields":{"slug":"/posts/hadoop/"},"frontmatter":{"title":"Hadoop"}}},{"node":{"id":"27ea8f29-94a3-5302-a637-efd0af842208"},"next":{"fields":{"slug":"/posts/java-script-3-제어흐름과-에러처리-loop-함수-표현식과-연산자-숫자와-날짜-텍스트-서식-정규-표현식/"},"frontmatter":{"title":"JavaScript (3) - 제어흐름과 에러처리, Loop, 함수, 표현식과 연산자, 숫자와 날짜, 텍스트 서식, 정규 표현식"}},"previous":{"fields":{"slug":"/posts/java-script-1-개요/"},"frontmatter":{"title":"JavaScript (1) - 개요"}}},{"node":{"id":"7470f1ce-f71d-5772-a8be-806ed3f7cc66"},"next":{"fields":{"slug":"/posts/java-script-4-loop와-반복-함수-표현식과-연산자-숫자와-날자-텍스트-서식-정규표현식/"},"frontmatter":{"title":"JavaScript (4) - Loop와 반복, 함수, 표현식과 연산자, 숫자와 날자, 텍스트 서식, 정규표현식"}},"previous":{"fields":{"slug":"/posts/java-script-2-문법과-자료형/"},"frontmatter":{"title":"JavaScript (2) - 문법과 자료형"}}},{"node":{"id":"c54308e0-d7a2-5d1c-b8ae-ecbfd04ed1ac"},"next":{"fields":{"slug":"/posts/java-script-5-collections/"},"frontmatter":{"title":"JavaScript (5) - Collections"}},"previous":{"fields":{"slug":"/posts/java-script-3-제어흐름과-에러처리-loop-함수-표현식과-연산자-숫자와-날짜-텍스트-서식-정규-표현식/"},"frontmatter":{"title":"JavaScript (3) - 제어흐름과 에러처리, Loop, 함수, 표현식과 연산자, 숫자와 날짜, 텍스트 서식, 정규 표현식"}}},{"node":{"id":"c8b65dec-7fef-5ef0-a515-27ae182dff3e"},"next":{"fields":{"slug":"/posts/java-script-6-objects/"},"frontmatter":{"title":"JavaScript (6) - Objects"}},"previous":{"fields":{"slug":"/posts/java-script-4-loop와-반복-함수-표현식과-연산자-숫자와-날자-텍스트-서식-정규표현식/"},"frontmatter":{"title":"JavaScript (4) - Loop와 반복, 함수, 표현식과 연산자, 숫자와 날자, 텍스트 서식, 정규표현식"}}},{"node":{"id":"6814abc2-41a1-520c-bd24-a8d2d02424fb"},"next":{"fields":{"slug":"/posts/java-script-7-반복기-및-생성기-메타-프로그래밍/"},"frontmatter":{"title":"JavaScript (7) - 반복기 및 생성기, 메타 프로그래밍"}},"previous":{"fields":{"slug":"/posts/java-script-5-collections/"},"frontmatter":{"title":"JavaScript (5) - Collections"}}},{"node":{"id":"2fa87adf-6f94-54db-a8e4-7ab4990ba5e6"},"next":{"fields":{"slug":"/posts/restful-api/"},"frontmatter":{"title":"Restful API"}},"previous":{"fields":{"slug":"/posts/java-script-6-objects/"},"frontmatter":{"title":"JavaScript (6) - Objects"}}},{"node":{"id":"54319bda-39af-5fa4-9eee-7f84ee580c1a"},"next":{"fields":{"slug":"/posts/ci-cd/"},"frontmatter":{"title":"CI/CD"}},"previous":{"fields":{"slug":"/posts/java-script-7-반복기-및-생성기-메타-프로그래밍/"},"frontmatter":{"title":"JavaScript (7) - 반복기 및 생성기, 메타 프로그래밍"}}},{"node":{"id":"2311c1d2-b785-582e-a9a9-d80750b5d505"},"next":{"fields":{"slug":"/posts/annotation/"},"frontmatter":{"title":"Annotation"}},"previous":{"fields":{"slug":"/posts/restful-api/"},"frontmatter":{"title":"Restful API"}}},{"node":{"id":"2e8c37a8-a44a-5356-b617-dc7b22034c2a"},"next":{"fields":{"slug":"/posts/di와-자동-di/"},"frontmatter":{"title":"DI와 자동 DI"}},"previous":{"fields":{"slug":"/posts/ci-cd/"},"frontmatter":{"title":"CI/CD"}}},{"node":{"id":"443ae298-350d-531c-a42c-34e0acb0cdd8"},"next":{"fields":{"slug":"/posts/spring-mvc-pattern/"},"frontmatter":{"title":"Spring MVC Pattern"}},"previous":{"fields":{"slug":"/posts/annotation/"},"frontmatter":{"title":"Annotation"}}},{"node":{"id":"64980158-2ad8-5c44-91a8-a0185a6d4f6b"},"next":{"fields":{"slug":"/posts/orm과-jpa-2/"},"frontmatter":{"title":"ORM과 JPA (2)"}},"previous":{"fields":{"slug":"/posts/di와-자동-di/"},"frontmatter":{"title":"DI와 자동 DI"}}},{"node":{"id":"5128821c-b701-5848-9979-bb91ec9c5836"},"next":{"fields":{"slug":"/posts/orm과-jpa-1/"},"frontmatter":{"title":"ORM과 JPA (1)"}},"previous":{"fields":{"slug":"/posts/spring-mvc-pattern/"},"frontmatter":{"title":"Spring MVC Pattern"}}},{"node":{"id":"d4549afd-0095-5810-b492-b843ca6e2f88"},"next":{"fields":{"slug":"/posts/apache-spark-1-개요/"},"frontmatter":{"title":"Apache Spark (1) - 개요"}},"previous":{"fields":{"slug":"/posts/orm과-jpa-2/"},"frontmatter":{"title":"ORM과 JPA (2)"}}},{"node":{"id":"46426481-29ee-5d00-aa6c-cf05d52490f6"},"next":{"fields":{"slug":"/posts/동시성-처리/"},"frontmatter":{"title":"동시성 처리"}},"previous":{"fields":{"slug":"/posts/orm과-jpa-1/"},"frontmatter":{"title":"ORM과 JPA (1)"}}},{"node":{"id":"6eed9dfa-e294-5b19-a225-cd43d6cb982a"},"next":{"fields":{"slug":"/posts/apache-spark-2-rdd/"},"frontmatter":{"title":"Apache Spark (2) - RDD"}},"previous":{"fields":{"slug":"/posts/apache-spark-1-개요/"},"frontmatter":{"title":"Apache Spark (1) - 개요"}}},{"node":{"id":"e46642ac-6d65-51b8-a535-21a9e3eabb56"},"next":{"fields":{"slug":"/posts/database/"},"frontmatter":{"title":"DATABASE"}},"previous":{"fields":{"slug":"/posts/동시성-처리/"},"frontmatter":{"title":"동시성 처리"}}},{"node":{"id":"f77c8cbf-3a58-53fc-bd32-592702676a11"},"next":{"fields":{"slug":"/posts/erd/"},"frontmatter":{"title":"ERD"}},"previous":{"fields":{"slug":"/posts/apache-spark-2-rdd/"},"frontmatter":{"title":"Apache Spark (2) - RDD"}}},{"node":{"id":"f016fc75-8476-5493-8ac1-e2862773f3e7"},"next":{"fields":{"slug":"/posts/git-사용법/"},"frontmatter":{"title":"Git 사용법"}},"previous":{"fields":{"slug":"/posts/database/"},"frontmatter":{"title":"DATABASE"}}},{"node":{"id":"9f5a9a91-7af6-5ca3-b20f-d6e177e46c68"},"next":{"fields":{"slug":"/posts/jpa-1-query/"},"frontmatter":{"title":"JPA (1) - Query"}},"previous":{"fields":{"slug":"/posts/erd/"},"frontmatter":{"title":"ERD"}}},{"node":{"id":"b4e07799-70a4-523d-90ec-741e18230194"},"next":{"fields":{"slug":"/posts/j-query/"},"frontmatter":{"title":"jQuery"}},"previous":{"fields":{"slug":"/posts/git-사용법/"},"frontmatter":{"title":"Git 사용법"}}},{"node":{"id":"ef05bf06-2d53-5323-820b-98f46a906289"},"next":{"fields":{"slug":"/posts/jpa-2-specification/"},"frontmatter":{"title":"JPA (2) - Specification"}},"previous":{"fields":{"slug":"/posts/jpa-1-query/"},"frontmatter":{"title":"JPA (1) - Query"}}},{"node":{"id":"571df319-d1dd-59b6-a588-7a5db12c6b3d"},"next":{"fields":{"slug":"/posts/kubernetes/"},"frontmatter":{"title":"Kubernetes"}},"previous":{"fields":{"slug":"/posts/j-query/"},"frontmatter":{"title":"jQuery"}}},{"node":{"id":"a78eb59a-78b3-5c86-bcc2-f09ea58fab23"},"next":{"fields":{"slug":"/posts/node-js/"},"frontmatter":{"title":"NodeJS"}},"previous":{"fields":{"slug":"/posts/jpa-2-specification/"},"frontmatter":{"title":"JPA (2) - Specification"}}},{"node":{"id":"3c37f3d1-7097-5528-a31b-12bcdcd937ab"},"next":{"fields":{"slug":"/posts/oop/"},"frontmatter":{"title":"OOP"}},"previous":{"fields":{"slug":"/posts/kubernetes/"},"frontmatter":{"title":"Kubernetes"}}},{"node":{"id":"fe690dd1-ec0c-544b-a44f-131d80987565"},"next":{"fields":{"slug":"/posts/postgre-sql-1-개요/"},"frontmatter":{"title":"PostgreSQL (1) - 개요"}},"previous":{"fields":{"slug":"/posts/node-js/"},"frontmatter":{"title":"NodeJS"}}},{"node":{"id":"6268f141-e6af-5f89-a55b-2174ef57651f"},"next":{"fields":{"slug":"/posts/postgre-sql-2-psql/"},"frontmatter":{"title":"PostgreSQL (2) - PSQL"}},"previous":{"fields":{"slug":"/posts/oop/"},"frontmatter":{"title":"OOP"}}},{"node":{"id":"b8c8c26f-09bc-5759-8a7e-c81ffbc8f2c0"},"next":{"fields":{"slug":"/posts/react/"},"frontmatter":{"title":"React"}},"previous":{"fields":{"slug":"/posts/postgre-sql-1-개요/"},"frontmatter":{"title":"PostgreSQL (1) - 개요"}}},{"node":{"id":"1a663d51-139e-500f-a209-04faaebd2e85"},"next":{"fields":{"slug":"/posts/react-hook/"},"frontmatter":{"title":"React Hook"}},"previous":{"fields":{"slug":"/posts/postgre-sql-2-psql/"},"frontmatter":{"title":"PostgreSQL (2) - PSQL"}}},{"node":{"id":"b227a2c1-7853-5450-a1be-f3e1c83307b4"},"next":{"fields":{"slug":"/posts/redux-react-redux/"},"frontmatter":{"title":"Redux & React-Redux"}},"previous":{"fields":{"slug":"/posts/react/"},"frontmatter":{"title":"React"}}},{"node":{"id":"80979747-a7b7-5e5d-81bb-30729e2e16bd"},"next":{"fields":{"slug":"/posts/react/"},"frontmatter":{"title":"React"}},"previous":{"fields":{"slug":"/posts/react-hook/"},"frontmatter":{"title":"React Hook"}}},{"node":{"id":"a82c048c-1eea-54e3-912d-ae875640e04e"},"next":{"fields":{"slug":"/posts/swagger/"},"frontmatter":{"title":"Swagger"}},"previous":{"fields":{"slug":"/posts/redux-react-redux/"},"frontmatter":{"title":"Redux & React-Redux"}}},{"node":{"id":"18c2a54b-1a71-511c-824c-f437be00f3ef"},"next":{"fields":{"slug":"/posts/web-rtc/"},"frontmatter":{"title":"WebRTC"}},"previous":{"fields":{"slug":"/posts/react/"},"frontmatter":{"title":"React"}}},{"node":{"id":"d54a4c19-013d-538a-893d-ff8f62d34d7a"},"next":{"fields":{"slug":"/posts/tomcat-사용법/"},"frontmatter":{"title":"Tomcat 사용법"}},"previous":{"fields":{"slug":"/posts/swagger/"},"frontmatter":{"title":"Swagger"}}},{"node":{"id":"d2056881-058f-5f23-88ec-cf613dff77f0"},"next":null,"previous":{"fields":{"slug":"/posts/web-rtc/"},"frontmatter":{"title":"WebRTC"}}}]}},"pageContext":{"id":"46426481-29ee-5d00-aa6c-cf05d52490f6"}},"staticQueryHashes":["2001315320"]}